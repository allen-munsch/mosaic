# Mosaic Refactoring Plan

**Goal:** Production-ready federated semantic search with full SQL capabilities. Position: "Vector search you can query with SQL. Ship as files."

## Phase 1: Foundation (Weeks 1-2)

### 1.1 Replace sqlite-vss with sqlite-vec

sqlite-vss is unmaintained. sqlite-vec is the successor by the same author, actively developed.

**Changes:**
- Replace `SqliteVss` dependency with `sqlite-vec`
- Update `StorageManager.create_schema/1`:
```elixir
# Old: CREATE VIRTUAL TABLE vss_vectors using vss0(vec(384))
# New: CREATE VIRTUAL TABLE vec_vectors USING vec0(embedding float[384])
```
- Update all vector operations to use `vec_distance_cosine()` instead of `vss_search()`
- Benefits: Better performance, SIMD support, actively maintained

**Files affected:**
- `lib/mosaic/storage_manager.ex`
- `lib/mosaic/query_engine.ex`
- `lib/mosaic/indexer.ex`
- `mix.exs` (dependency swap)

### 1.2 Fix Schema Design

Current schema is fragile. Consolidate:

```sql
CREATE TABLE documents (
  id TEXT PRIMARY KEY,
  text TEXT NOT NULL,
  metadata JSON,
  embedding BLOB,           -- Store raw embedding alongside
  created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
  updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
  pagerank REAL DEFAULT 0.0,
  word_count INTEGER GENERATED ALWAYS AS (length(text) - length(replace(text, ' ', '')) + 1) STORED
);

CREATE INDEX idx_docs_created ON documents(created_at);
CREATE INDEX idx_docs_pagerank ON documents(pagerank DESC);

CREATE VIRTUAL TABLE vec_documents USING vec0(
  id TEXT PRIMARY KEY,
  embedding float[384]
);
```

### 1.3 Connection Management Overhaul

Current `Resilience` module is basic. Implement proper pooling:

```elixir
defmodule Mosaic.ConnectionPool do
  use GenServer
  
  defstruct [:pools, :config]
  
  # Per-shard pools with:
  # - Configurable pool size
  # - Connection health checks
  # - Automatic reconnection
  # - WAL checkpoint management
  # - Busy timeout handling
end
```

Add SQLite pragmas for concurrent access:
```sql
PRAGMA busy_timeout = 5000;
PRAGMA wal_autocheckpoint = 1000;
PRAGMA journal_size_limit = 67108864;
```

---

## Phase 2: Federated Query Engine (Weeks 3-4)

This is your differentiator. Make it exceptional.

### 2.1 Query Planner with Predicate Pushdown

Current `FederatedQuery` sends identical SQL to all shards. Optimize:

```elixir
defmodule Mosaic.QueryPlanner do
  @moduledoc "Analyzes SQL and optimizes shard selection"
  
  defstruct [:parsed_query, :predicates, :target_shards, :execution_plan]
  
  def plan(sql, params, opts \\ []) do
    sql
    |> parse()
    |> extract_predicates()
    |> select_shards()      # Use bloom filters + metadata
    |> optimize_execution()  # Parallel vs sequential
    |> build_plan()
  end
  
  # If query has "WHERE domain = 'example.com'", only hit shards
  # whose bloom filter might contain that domain
  defp select_shards(%{predicates: preds} = plan) do
    candidate_shards = Mosaic.ShardRouter.list_all_shards()
    
    filtered = candidate_shards
    |> filter_by_bloom(preds)
    |> filter_by_metadata(preds)  # date ranges, etc.
    
    %{plan | target_shards: filtered}
  end
end
```

### 2.2 Hybrid Vector + SQL Queries

The killer feature: combine semantic search with SQL filtering in one query.

```elixir
defmodule Mosaic.HybridQuery do
  @doc """
  Search semantically with SQL filters pushed down.
  
  Example:
    HybridQuery.search(
      "machine learning optimization",
      where: "created_at > '2024-01-01' AND metadata->>'category' = 'research'",
      limit: 20
    )
  """
  def search(query_text, opts \\ []) do
    embedding = Mosaic.EmbeddingService.encode(query_text)
    sql_filter = Keyword.get(opts, :where)
    limit = Keyword.get(opts, :limit, 20)
    
    # Build optimized query that does vector search + SQL filter in one pass
    shards = select_shards_for_query(query_text, sql_filter)
    
    shards
    |> Task.async_stream(&search_shard(&1, embedding, sql_filter, limit * 2))
    |> collect_and_rank(limit)
  end
  
  defp search_shard(shard, embedding, sql_filter, limit) do
    # Single query combining vector distance with SQL predicates
    sql = """
    SELECT d.*, vec_distance_cosine(v.embedding, ?) as distance
    FROM documents d
    JOIN vec_documents v ON d.id = v.id
    WHERE #{sql_filter || "1=1"}
    ORDER BY distance ASC
    LIMIT ?
    """
    execute(shard, sql, [embedding, limit])
  end
end
```

### 2.3 Aggregation Engine

Federated aggregations with proper semantics:

```elixir
defmodule Mosaic.Aggregator do
  @doc "Execute aggregation across shards with correct merge semantics"
  
  def aggregate(sql, params \\ []) do
    plan = QueryPlanner.plan(sql, params)
    
    case detect_aggregation_type(plan) do
      :count -> sum_results(execute_all(plan))
      :sum -> sum_results(execute_all(plan))
      :avg -> weighted_average(execute_all(plan))
      :min -> min_results(execute_all(plan))
      :max -> max_results(execute_all(plan))
      :group_by -> merge_groups(execute_all(plan))
      :distinct -> union_distinct(execute_all(plan))
    end
  end
  
  # GROUP BY requires merging partial results
  defp merge_groups(shard_results) do
    shard_results
    |> Enum.flat_map(& &1)
    |> Enum.group_by(&group_key/1)
    |> Enum.map(&re_aggregate/1)
  end
end
```

---

## Phase 3: Ranking & Relevance (Weeks 5-6)

### 3.1 Implement Real PageRank

Current implementation is a stub. Fix it:

```elixir
defmodule Mosaic.PageRank do
  @doc "Compute PageRank across document link graph"
  
  def compute(opts \\ []) do
    iterations = Keyword.get(opts, :iterations, 20)
    damping = Keyword.get(opts, :damping, 0.85)
    
    # Extract link graph from documents
    graph = build_link_graph()
    n = map_size(graph)
    
    # Initialize ranks
    initial_rank = 1.0 / n
    ranks = Map.new(Map.keys(graph), fn id -> {id, initial_rank} end)
    
    # Iterate
    final_ranks = Enum.reduce(1..iterations, ranks, fn _, current_ranks ->
      iterate(graph, current_ranks, damping, n)
    end)
    
    # Persist to shards
    persist_ranks(final_ranks)
  end
  
  defp build_link_graph do
    # Query all documents for outbound links in metadata
    Mosaic.FederatedQuery.execute("""
      SELECT id, json_extract(metadata, '$.outbound_links') as links
      FROM documents
      WHERE json_extract(metadata, '$.outbound_links') IS NOT NULL
    """)
    |> Enum.reduce(%{}, fn [id, links_json], acc ->
      links = Jason.decode!(links_json || "[]")
      Map.put(acc, id, links)
    end)
  end
  
  defp iterate(graph, ranks, damping, n) do
    base = (1 - damping) / n
    
    Map.new(ranks, fn {id, _} ->
      inbound = get_inbound_links(graph, id)
      sum = Enum.reduce(inbound, 0.0, fn from_id, acc ->
        from_rank = Map.get(ranks, from_id, 0)
        out_degree = length(Map.get(graph, from_id, []))
        acc + from_rank / max(out_degree, 1)
      end)
      {id, base + damping * sum}
    end)
  end
end
```

### 3.2 BM25 Text Scoring

Add proper lexical scoring alongside vector similarity:

```elixir
defmodule Mosaic.Ranking.Scorers.BM25 do
  @behaviour Mosaic.Ranking.Scorer
  
  @k1 1.2
  @b 0.75
  
  @impl true
  def name, do: :bm25
  
  @impl true
  def score(%{text: text, word_count: doc_len}, %{query_terms: terms, corpus_stats: stats}) do
    avg_len = stats.avg_doc_length
    
    terms
    |> Enum.map(fn term ->
      tf = term_frequency(text, term)
      idf = inverse_doc_frequency(term, stats)
      numerator = tf * (@k1 + 1)
      denominator = tf + @k1 * (1 - @b + @b * (doc_len / avg_len))
      idf * (numerator / denominator)
    end)
    |> Enum.sum()
    |> normalize()
  end
  
  @impl true
  def weight, do: 0.15
end
```

### 3.3 Configurable Ranking Pipelines

Allow users to define custom ranking:

```elixir
# In config or at query time
ranker = Mosaic.Ranking.Ranker.new(
  scorers: [
    {Scorers.VectorSimilarity, weight: 0.5},
    {Scorers.BM25, weight: 0.2},
    {Scorers.PageRank, weight: 0.15},
    {Scorers.Freshness, weight: 0.1, half_life_days: 14},
    {Scorers.Custom, weight: 0.05, function: &boost_by_source/2}
  ],
  fusion: :rrf,  # or :weighted_sum, :max
  min_score: 0.1
)

Mosaic.Search.perform_search("query", ranker: ranker)
```

---

## Phase 4: Shard Management (Weeks 7-8)

### 4.1 Intelligent Shard Routing

Replace centroid-only routing with multi-signal approach:

```elixir
defmodule Mosaic.ShardSelector do
  @doc "Select shards using multiple signals"
  
  def select(query_embedding, query_terms, sql_predicates, opts) do
    all_shards = list_shards()
    
    all_shards
    |> score_by_centroid(query_embedding)      # Semantic relevance
    |> filter_by_bloom(query_terms)            # Term presence
    |> filter_by_metadata(sql_predicates)      # Date ranges, domains, etc.
    |> filter_by_partition_key(opts)           # Tenant isolation
    |> rank_by_query_history()                 # Hot shards first
    |> take(opts[:shard_limit] || 10)
  end
  
  defp filter_by_metadata(shards, predicates) do
    # Each shard stores min/max created_at, list of domains, etc.
    # Skip shards that can't possibly match
    Enum.filter(shards, fn shard ->
      Enum.all?(predicates, &shard_might_match?(shard.metadata, &1))
    end)
  end
end
```

### 4.2 Shard Lifecycle Management

```elixir
defmodule Mosaic.ShardManager do
  @doc "Manage shard creation, compaction, archival"
  
  # Auto-create new shard when current exceeds size limit
  def maybe_rotate_shard(current_shard) do
    if get_doc_count(current_shard) >= @max_shard_size do
      new_shard = create_shard()
      set_active_shard(new_shard)
      schedule_compaction(current_shard)
      {:rotated, new_shard}
    else
      {:ok, current_shard}
    end
  end
  
  # Compact: VACUUM, rebuild indexes, update centroid
  def compact(shard_path) do
    with {:ok, conn} <- checkout(shard_path) do
      execute(conn, "VACUUM")
      execute(conn, "REINDEX")
      execute(conn, "ANALYZE")
      update_shard_metadata(shard_path)
      checkin(shard_path, conn)
    end
  end
  
  # Archive cold shards to cheaper storage
  def archive(shard_path, destination) do
    # Copy to S3/GCS, update routing to mark as archived
    # Archived shards can still be queried but with higher latency
  end
end
```

### 4.3 Multi-Tenancy via Partition Keys

```elixir
defmodule Mosaic.Tenancy do
  @doc "Isolate tenants into separate shard sets"
  
  def index_document(tenant_id, doc_id, text, metadata) do
    shard = get_or_create_tenant_shard(tenant_id)
    Mosaic.Indexer.index_document(doc_id, text, metadata, shard: shard)
  end
  
  def search(tenant_id, query, opts) do
    tenant_shards = get_tenant_shards(tenant_id)
    Mosaic.Search.perform_search(query, Keyword.put(opts, :shards, tenant_shards))
  end
  
  # Each tenant's data is physically isolated in separate SQLite files
  # Easy backup, deletion, migration per tenant
end
```

---

## Phase 5: API & SDK (Weeks 9-10)

### 5.1 RESTful API Expansion

```elixir
defmodule Mosaic.API do
  # Search
  post "/api/v1/search" # Semantic search
  post "/api/v1/search/hybrid" # Vector + SQL
  post "/api/v1/search/sql" # Pure SQL federated
  
  # Indexing
  post "/api/v1/documents" # Single doc
  post "/api/v1/documents/batch" # Batch upsert
  delete "/api/v1/documents/:id"
  patch "/api/v1/documents/:id" # Partial update
  
  # Collections (shard groups)
  post "/api/v1/collections"
  get "/api/v1/collections/:name/stats"
  post "/api/v1/collections/:name/compact"
  
  # Admin
  get "/api/v1/health"
  get "/api/v1/metrics"
  post "/api/v1/admin/reindex"
  post "/api/v1/admin/compute-pagerank"
end
```

### 5.2 Client SDKs

Start with Elixir client, then Python (biggest market):

```python
# Python SDK example
from mosaic import MosaicClient

client = MosaicClient("http://localhost:4040")

# Semantic search
results = client.search("machine learning papers", limit=10)

# Hybrid search
results = client.search(
    "neural networks",
    where="created_at > '2024-01-01' AND metadata->>'venue' = 'NeurIPS'",
    limit=20
)

# Federated SQL
stats = client.sql("""
    SELECT metadata->>'author' as author, COUNT(*) as papers
    FROM documents
    GROUP BY author
    ORDER BY papers DESC
    LIMIT 10
""")

# Index documents
client.index([
    {"id": "doc1", "text": "...", "metadata": {"author": "..."}},
    {"id": "doc2", "text": "...", "metadata": {"author": "..."}},
])
```

### 5.3 Streaming Results

For large result sets:

```elixir
defmodule Mosaic.Streaming do
  def stream_search(query, opts) do
    Stream.resource(
      fn -> init_search(query, opts) end,
      fn state -> fetch_next_batch(state) end,
      fn state -> cleanup(state) end
    )
  end
  
  # API endpoint
  get "/api/v1/search/stream" do
    conn
    |> put_resp_content_type("application/x-ndjson")
    |> send_chunked(200)
    |> stream_results(query, opts)
  end
end
```

---

## Phase 6: Reliability & Observability (Weeks 11-12)

### 6.1 Comprehensive Testing

```
test/
├── unit/
│   ├── ranking/
│   │   ├── scorer_test.exs
│   │   ├── fusion_test.exs
│   │   └── ranker_test.exs
│   ├── query_planner_test.exs
│   └── shard_selector_test.exs
├── integration/
│   ├── federated_query_test.exs
│   ├── hybrid_search_test.exs
│   └── multi_tenant_test.exs
├── performance/
│   ├── indexing_bench.exs
│   ├── search_latency_bench.exs
│   └── concurrent_load_test.exs
└── property/
    └── query_correctness_test.exs  # StreamData for fuzzing
```

### 6.2 Metrics & Tracing

```elixir
defmodule Mosaic.Metrics do
  use PromEx, otp_app: :mosaic
  
  def plugins do
    [
      Mosaic.Metrics.SearchPlugin,   # Query latency, result counts
      Mosaic.Metrics.IndexPlugin,    # Indexing throughput
      Mosaic.Metrics.ShardPlugin,    # Shard sizes, query distribution
      Mosaic.Metrics.CachePlugin,    # Hit rates
    ]
  end
end

# OpenTelemetry tracing
defmodule Mosaic.Tracing do
  require OpenTelemetry.Tracer, as: Tracer
  
  def trace_search(query, opts, fun) do
    Tracer.with_span "mosaic.search" do
      Tracer.set_attributes([
        {"query.length", String.length(query)},
        {"query.limit", opts[:limit]}
      ])
      
      {time, result} = :timer.tc(fun)
      
      Tracer.set_attributes([
        {"search.duration_ms", time / 1000},
        {"search.result_count", length(result)}
      ])
      
      result
    end
  end
end
```

### 6.3 Error Handling & Recovery

```elixir
defmodule Mosaic.Recovery do
  @doc "Handle shard failures gracefully"
  
  def search_with_fallback(shards, query_fn) do
    shards
    |> Task.async_stream(fn shard ->
      try do
        {:ok, query_fn.(shard)}
      rescue
        e -> 
          Logger.warning("Shard #{shard.id} failed: #{inspect(e)}")
          mark_shard_unhealthy(shard.id)
          {:error, shard.id}
      end
    end, on_timeout: :kill_task, timeout: 5000)
    |> Enum.reduce({[], []}, fn
      {:ok, {:ok, results}}, {good, bad} -> {[results | good], bad}
      {:ok, {:error, id}}, {good, bad} -> {good, [id | bad]}
      {:exit, _}, {good, bad} -> {good, bad}
    end)
    |> then(fn {results, failed} ->
      if length(failed) > 0 do
        Logger.warning("Search degraded, #{length(failed)} shards failed")
      end
      List.flatten(results)
    end)
  end
end
```

---

## Phase 7: Documentation & Launch (Week 13+)

### 7.1 Documentation

```
docs/
├── getting-started.md
├── architecture.md
├── api-reference.md
├── sql-reference.md        # Supported SQL syntax
├── ranking.md              # How scoring works
├── deployment/
│   ├── docker.md
│   ├── kubernetes.md
│   └── fly-io.md
├── guides/
│   ├── multi-tenancy.md
│   ├── hybrid-search.md
│   └── custom-ranking.md
└── benchmarks.md
```

### 7.2 Deployment Artifacts

```dockerfile
# Dockerfile
FROM hexpm/elixir:1.16.0-erlang-26.2-debian-bookworm-20231009 AS build
# ... build steps

FROM debian:bookworm-slim
COPY --from=build /app/_build/prod/rel/mosaic ./
EXPOSE 4040
CMD ["bin/mosaic", "start"]
```

```yaml
# docker-compose.yml for local dev
services:
  mosaic:
    build: .
    ports:
      - "4040:4040"
    volumes:
      - ./data:/data
    environment:
      - STORAGE_PATH=/data/shards
      - EMBEDDING_MODEL=local
```

### 7.3 Benchmarks

Publish honest benchmarks against:
- pgvector (your main competitor)
- Typesense (if they want hybrid)
- Meilisearch (full-text focused)

Focus on:
- Hybrid query latency (your strength)
- SQL aggregation across vectors (unique)
- Storage efficiency
- Cold start time (SQLite wins here)

---

## Priority Order

If time-constrained, execute in this order:

1. **sqlite-vec migration** - Foundation, unblocks everything
2. **HybridQuery module** - Your differentiator
3. **Query planner with predicate pushdown** - Makes hybrid fast
4. **Connection pooling fix** - Required for production load
5. **API expansion** - Users need to access features
6. **BM25 scorer** - Major relevance improvement
7. **Multi-tenancy** - Enterprise requirement
8. **Real PageRank** - Nice to have
9. **Observability** - Debug production issues

---

## Success Metrics

Before calling it "production ready":

- [ ] p99 search latency < 100ms for 10M docs across 100 shards
- [ ] Hybrid queries with SQL filters show <20% overhead vs pure vector
- [ ] Zero data loss under ungraceful shutdown (WAL works)
- [ ] 1000 concurrent queries without degradation
- [ ] Documentation covers all public APIs
- [ ] Published benchmarks against pgvector
